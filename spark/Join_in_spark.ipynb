{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f8a006-b38f-403a-93bb-d6ee998de032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64af0be-e695-41c5-a3da-6c88aa32d03c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_data = [\n",
    "    (1,'manish','patna',\"30-05-2022\"),\n",
    "    (2,'vikash','kolkata',\"12-03-2023\"),\n",
    "    (3,'nikita','delhi',\"25-06-2023\"),\n",
    "    (4,'rahul','ranchi',\"24-03-2023\"),\n",
    "    (5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "    (6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "    (7,'raman','patna',\"30-12-2022\"),\n",
    "    (8,'prakash','ranchi',\"24-02-2023\"),\n",
    "    (9,'ragini','kolkata',\"03-03-2023\"),\n",
    "    (10,'raushan','jaipur',\"05-02-2023\")\n",
    "]\n",
    "\n",
    "customer_schema = ['customer_id','customer_name','address','date_of_joining']\n",
    "\n",
    "customer_df = spark.createDataFrame(customer_data, schema=customer_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e59a7f-2a51-45f4-b385-36a6c8fe69d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_data = [\n",
    "    (1,22,10,\"01-06-2022\"),\n",
    "    (1,27,5,\"03-02-2023\"),\n",
    "    (2,5,3,\"01-06-2023\"),\n",
    "    (5,22,1,\"22-03-2023\"),\n",
    "    (7,22,4,\"03-02-2023\"),\n",
    "    (9,5,6,\"03-03-2023\"),\n",
    "    (2,1,12,\"15-06-2023\"),\n",
    "    (1,56,2,\"25-06-2023\"),\n",
    "    (5,12,5,\"15-04-2023\"),\n",
    "    (11,12,76,\"12-03-2023\")\n",
    "]\n",
    "\n",
    "sales_schema = ['customer_id','product_id','quantity','date_of_purchase']\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, schema=sales_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b330f3-8d48-4bad-8a9f-69b72994a78e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_data = [\n",
    "    (1, 'fanta',20),\n",
    "    (2, 'dew',22),\n",
    "    (5, 'sprite',40),\n",
    "    (7, 'redbull',100),\n",
    "    (12,'mazza',45),\n",
    "    (22,'coke',27),\n",
    "    (25,'limca',21),\n",
    "    (27,'pepsi',14),\n",
    "    (56,'sting',10)\n",
    "]\n",
    "\n",
    "product_schema = ['id','name','price']\n",
    "\n",
    "product_df = spark.createDataFrame(product_data, schema=product_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29510cb3-145f-44bc-9e67-06fd2b660cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4957e6-302e-44c1-8155-1103dfa7ddb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b8deca-7a63-4600-9b1c-5dea7cf7ec8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df.join(sales_df,customer_df['customer_id'] == sales_df['customer_id'],\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de4b5fd9-9a7f-401c-8c9d-e81398a71a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Then All the same types of joins in sql \n",
    "Consider using SQL for Joins as pyspark is somewhat complex to write \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "309a0ddd-e4d2-4961-8cfe-3e319f81196d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Join_in_spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
